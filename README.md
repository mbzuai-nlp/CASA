<h1 align="center"> Clinical Annoations for Automatic Stuttering Assessment </h1>

<div align="center">

[![License: MIT](https://img.shields.io/badge/license-MIT-green)](LICENSE)
[![Paper](https://img.shields.io/badge/Paper-PDF-blue)](https://arxiv.org/abs/2505.15425)
[![Project](https://img.shields.io/badge/Project-CASA-red)](https://github.com/mbzuai-nlp/CASA)
</div>


> **Accepted at Interspeech2025]**

This project contains the dataset  and baseline multimodal classification models described by [Paper](link to paper) Refer  [Annotation Guidline](guidlines.md) to see the details of the annotaion guidlines.


## Project Structure

```
project/
â””â”€â”€ data/
    â””â”€â”€ Voices-AWS/
        â”œâ”€â”€ interview/
        â”‚   â”œâ”€â”€ video/                # Place MP4 files here
        â”‚   â”œâ”€â”€ total_dataset.csv     # Annotation data
        â”‚   â”œâ”€â”€ exclusions.csv        # Optional: segments to exclude
        â”‚   â””â”€â”€ raw.csv               # Raw data
        â””â”€â”€ reading/
            â”œâ”€â”€ video/                # Place MP4 files here
            â”œâ”€â”€ total_dataset.csv     # Annotation data
            â”œâ”€â”€ exclusions.csv        # Optional: segments to exclude
            â””â”€â”€ raw.csv    

```

## ğŸ“¦ Installation

```bash
git clone https://github.com/mbzuai-nlp/CASA.git
cd CASA
conda create -n casa python=3.12
conda activate casa
pip install -r requirements.txt
```

## ğŸ“ Data Preparation

### 1. Prepare Input Data

1. **Download Media Files**
   Follow the instructions [Here](docs/download.md)

2. **Verify Input Data Structure**:
   ```
   data/Voices-AWS/interview/
   â”œâ”€â”€ video/
   â”‚   â”œâ”€â”€ participant1.mp4
   â”‚   â”œâ”€â”€ participant2.mp4
   â”‚   â””â”€â”€ ...
   â”œâ”€â”€ total_dataset_final.csv
   â””â”€â”€ exclusions.csv
   ```

3. **Required Files**:
   - `total_dataset.csv`: Contains stuttering annotations with columns:
     - media_file: filename without extension
     - item: the group id in the form [media_id-(group_start_time, group_end_time)] after grouping annotations based on region.
     - start: start time in milliseconds
     - end: end time in milliseconds
     - annotator: (A1, A2, A3, Gold) and additional annotator aggrigation methods (BAU, MAS, SAD)
     - SR, ISR, MUR, P, B, V, FG, HM, ME: stuttering type indicators (0/1) refere to [Annotation Guidelines](docs/guidlines.md) for details
  - `exclusions.csv` : Containes the unannotated region. (Interviewer part of interview section)

### 2. Run Dataset Preparation
To prepare the data for processing with pyannote run the following command:
(Note: This takes ~ 30 mins with a 16 core CPU.)
```bash
python prepare.py \
    --root_dir "/path/to/root/dir" \ 
    --input_dir "/path/to/output/dir" \ 
    --clip_duration 5 \ # duration of each clip in seconds
    --overlap 2 \ # the overlap window
    --max_workers 8 \ # update this number based on the number of cpu cores
```

The script generates:
1. 5 second audio and video clips
2. Labels for each annotator
3. Labels for the aggrigation methods (BAU, MAS, SAD)

## ğŸ§  Models

To train the models, use the following command
```bash
python train.py \
    --modality audio \ # audio, video, multimodal
    --dataset_root "/path/to/dataset/dir" \
    --dataset_annotator "BAU" \ #eg annotator to use to train the models
    --output_dir "/path/to/output" \ 
```

## Notes

- Video files should be in MP4 format
- File names in the CSV should match the media files (without extension)
- Start/end times in the CSV should be in milliseconds


## âœï¸ Citation

If you find this data annotations helpful, please cite our paper:

```bibtex

```